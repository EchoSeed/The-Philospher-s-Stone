```json
{
  "name": "Neural Networks From Scratch",
  "description": "This skill activates when an AI agent needs to understand, explain, or implement neural networks at the foundational level—without relying on high-level frameworks like TensorFlow or PyTorch. It triggers during educational contexts where deep understanding of neural network mechanics is required, during debugging sessions where framework abstractions obscure root causes, when designing custom architectures that require manual control over computation graphs, or when explaining how neural networks actually work to learners who need to grasp the underlying mathematics and algorithms rather than just API calls.",
  "overview": "This skill guides the implementation of neural networks by building every component from mathematical first principles. It emphasizes mastering linear algebra for data transformations, calculus for optimization, and probability for handling uncertainty. The agent learns to manually construct forward passes that propagate data through layers, implement loss functions that quantify prediction errors, derive and code backpropagation algorithms that compute gradients, and systematically debug numerical issues. This hands-on, framework-free approach reveals the true computational mechanics of neural networks—exposing how matrix multiplications flow data forward, how chain rule applications flow gradients backward, and how iterative parameter updates drive learning. The skill prioritizes understanding over convenience, transparency over abstraction, and mathematical rigor over black-box tooling.",
  "when_to_use": [
    "Explaining neural network fundamentals to learners who need to understand what happens beneath framework abstractions",
    "Implementing custom neural architectures where standard frameworks impose limitations or hide critical details",
    "Debugging neural network training issues where framework complexity obscures the actual computational or mathematical problems",
    "Educational contexts requiring demonstration of forward propagation, backpropagation derivations, or gradient computation mechanics",
    "Situations demanding deep understanding of how matrix operations, activation functions, and loss gradients actually interact",
    "When numerical stability issues (NaNs, exploding/vanishing gradients) require inspection of raw computational steps",
    "Building intuition for how hyperparameters, initialization schemes, and architectural choices affect learning dynamics"
  ],
  "workflow": [
    "**Master the Mathematical Foundation**: Begin with linear algebra (vectors, matrices, matrix multiplication, transposes), calculus (derivatives, partial derivatives, chain rule), and probability (distributions, likelihood, expectation). These aren't optional prerequisites—they're the language in which neural networks are actually written.",
    "**Understand the Neuron**: Implement a single computational unit that performs z = w·x + b (weighted sum plus bias), then applies a nonlinear activation function f(z) to produce output. Test with simple inputs to verify the transformation behavior.",
    "**Build the Forward Pass**: Construct layer-by-layer data flow manually using matrix multiplication. For each layer i: compute A[i] = f(W[i] · A[i-1] + b[i]) where A is activation, W is weight matrix, b is bias vector, and f is activation function. Track and print tensor shapes at every step to catch dimension mismatches early.",
    "**Implement the Loss Function**: Hand-code the objective function that measures prediction error (e.g., mean squared error for regression: L = (1/n)Σ(ŷ - y)², or cross-entropy for classification: L = -Σ y·log(ŷ)). Compute loss on small test batches and verify it decreases when predictions improve.",
    "**Derive Backpropagation on Paper**: Work through the chain rule mathematics to derive gradient formulas for each layer. Starting from ∂L/∂ŷ (loss gradient with respect to predictions), apply chain rule backwards: ∂L/∂W[i] = ∂L/∂A[i] · ∂A[i]/∂z[i] · ∂z[i]/∂W[i]. Keep the derivation visible during implementation.",
    "**Implement Gradient Computation**: Code the backward pass that computes gradients for all parameters. Start from output layer and propagate error signals backward through each layer using the derived formulas. Store intermediate gradients for each weight matrix and bias vector.",
    "**Validate with Numerical Gradient Checking**: Implement finite difference approximation: (f(θ + ε) - f(θ - ε))/(2ε) ≈ ∂f/∂θ. Compare analytical gradients from backprop against numerical gradients to verify correctness. Even small discrepancies indicate implementation bugs.",
    "**Debug Using Tiny Data**: Test everything with minimal examples—2-3 data points, 2-3 features, tiny hidden layers (2-5 neurons). This makes all computations hand-verifiable and makes bugs obvious. Print everything: layer inputs, outputs, weights, gradients, loss values at each iteration.",
    "**Monitor for NaNs and Numerical Issues**: Check for Not-a-Number values after every operation. Common causes: division by zero in loss functions, log(0) in cross-entropy, exploding gradients from bad initialization, underflow in sigmoid/softmax. When NaNs appear, add print statements before the operation that produces them.",
    "**Verify Tensor Shape Compatibility**: Before every matrix multiplication or elementwise operation, assert that dimensions align correctly. Input shape [batch, features] × weight shape [features, hidden] = output shape [batch, hidden]. Shape mismatches are the most common implementation errors.",
    "**Implement Parameter Updates**: Apply gradient descent: θ_new = θ_old - learning_rate × gradient. Start with a small learning rate (0.001 - 0.01) to ensure stability. Verify that loss decreases over iterations—if it increases or oscillates wildly, reduce learning rate or check gradient signs.",
    "**Iterate Through Debug Cycles**: When something breaks (and it will), follow the systematic process: form hypothesis about bug location, add diagnostic prints around that area, run on minimal test case, identify the specific operation producing incorrect values, fix the implementation, verify fix with assertions, remove debug prints, re-test on full problem. Repeat until all tests pass.",
    "**Scale Up Gradually**: Once working on toy problems, incrementally increase complexity: more data points, more features, deeper networks, larger batches. At each scaling step, verify outputs remain sensible. This reveals issues like vanishing/exploding gradients that only manifest at scale."
  ],
  "key_patterns": [
    {
      "name": "Chain Rule is Everything",
      "detail": "Backpropagation is just repeated application of calculus chain rule: if y = f(g(x)), then dy/dx = (dy/dg)·(dg/dx). Every gradient computation breaks into local gradients multiplied together. Write out the chain for each layer before coding to prevent sign errors and missing terms."
    },
    {
      "name": "Shapes Before Code",
      "detail": "Before implementing any operation, write down the tensor shapes: Input[batch, in_features] × Weights[in_features, out_features] = Output[batch, out_features]. Dimension mismatches cause 80% of implementation bugs. Document expected shapes in comments and assert them in code."
    },
    {
      "name": "Start Simple, Scale Later",
      "detail": "Begin with the simplest possible case: 1 data point, 1 feature, 1 neuron, 1 layer. Verify this works perfectly before adding complexity. Test on data where you can manually compute the correct answer (e.g., XOR problem with 4 points). Complexity hides bugs; simplicity reveals them."
    },
    {
      "name": "Print-Driven Debugging",
      "detail": "Insert print statements everywhere during development: layer outputs, weight values, gradient magnitudes, loss at each iteration. Watch for patterns: gradients should be small but non-zero, loss should decrease monotonically, weights should change gradually. Sudden jumps or zeros indicate bugs."
    },
    {
      "name": "Numerical Gradient Checking",
      "detail": "Implement two-sided finite difference: (f(θ+ε) - f(θ-ε))/(2ε) for small ε (1e-5). Compute this for every parameter and compare against analytical gradients. Relative error should be <1e-7. This catches subtle math errors that produce plausible-looking but incorrect gradients."
    },
    {
      "name": "NaN Archaeology",
      "detail": "When NaNs appear, they propagate through all subsequent computations. Trace backwards to find the first operation producing NaN. Common sources: log(0) in cross-entropy (add epsilon smoothing), exp(large_number) overflow in softmax (subtract max before exp), division by zero in normalization (add epsilon to denominator)."
    },
    {
      "name": "Initialization Matters",
      "detail": "Random weight initialization scale affects learning. Too large → exploding gradients and NaNs. Too small → vanishing gradients and no learning. Use Xavier/He initialization: scale by sqrt(1/fan_in) or sqrt(2/fan_in). Test that activations and gradients maintain reasonable magnitudes across layers."
    },
    {
      "name": "Activation Function Characteristics",
      "detail": "Each activation has trade-offs: Sigmoid saturates (gradients → 0) for large |inputs|, killing learning. ReLU is simple but has dead neurons (gradient always 0 for negative inputs). Tanh is zero-centered but still saturates. Choose based on problem and monitor activation distributions during training."
    },
    {
      "name": "Learning Rate as Diagnostic",
      "detail": "Loss increasing → learning rate too high or wrong gradient signs. Loss plateauing immediately → learning rate too small or vanishing gradients. Loss oscillating → learning rate at edge of stability. Optimal learning rate shows smooth, consistent decrease. Start conservatively small and increase if learning is too slow."
    },
    {
      "name": "Batch vs Single Sample",
      "detail": "Implement both single-sample and batch operations. Single samples simplify debugging (can hand-verify every calculation). Batches require proper broadcasting and dimension handling but are essential for efficiency. Test single sample first, then extend to batches while checking that results match averaged single-sample results."
    }
  ],
  "edge_cases": [
    "**Vanishing Gradients in Deep Networks**: In networks with many layers, repeated multiplication of small gradient values (<1) during backpropagation causes gradients to approach zero in early layers, preventing learning. Mitigate with ReLU activations, residual connections, batch normalization, or gradient clipping. Test by printing gradient magnitudes at each layer—early layers should still have non-negligible gradients.",
    "**Exploding Gradients**: Repeated multiplication of large gradient values (>1) causes exponential growth, leading to NaN weights. Symptoms: loss suddenly becomes NaN, weight values shoot to infinity. Solutions: reduce learning rate, use gradient clipping (cap gradient norm at threshold like 1.0), improve weight initialization, or check for bugs in gradient computation that might be missing negative signs.",
    "**Loss Function Edge Cases**: Cross-entropy with log(0) produces -∞ and NaN. Add epsilon smoothing: log(p + 1e-10). Softmax with large logits overflows: subtract max logit before exponentiating (softmax(x) = softmax(x - max(x))). Mean squared error with unnormalized outputs can have massive magnitude—consider output scaling or normalization.",
    "**Dead ReLU Neurons**: If ReLU input is always negative during initialization, gradient is always zero, and that neuron never learns (\"dies\"). Monitor percentage of dead neurons (always zero activation). Prevent with careful initialization, leaky ReLU (small negative slope), or other activation functions. Re-initialize dead neurons if detected during training.",
    "**Matrix Dimension Mismatches**: Most common error. Input[10, 20] cannot multiply Weight[30, 40] without transpose. Broadcasting rules can hide bugs: [10, 1] + [10, 5] broadcasts to [10, 5] silently. Always explicitly check and document expected shapes. Use assertions: assert x.shape == (batch_size, n_features) before operations.",
    "**Learning Rate Too High**: Causes oscillation around minimum or divergence. Loss increases or jumps erratically. Weights update too drastically each step. Solution: reduce learning rate by factor of 10 and observe if loss curve smooths. For very sensitive problems, use learning rate schedules that start small and decrease over time.",
    "**Integer Division and Type Errors**: In languages like Python 2, 1/2 = 0 (integer division), not 0.5. Ensure floating-point arithmetic everywhere. Similarly, matrix operations require specific types—mixing int and float tensors can cause silent conversion or errors. Explicitly cast to float32/float64 at data loading stage.",
    "**Batch Size Effects**: Very small batches (1-2) give noisy gradient estimates and erratic learning. Very large batches give smooth but potentially poor final performance. Start with moderate batch sizes (32-128). Single-sample debugging reveals per-example issues, but batch behavior can differ due to averaging and broadcasting effects.",
    "**Gradient Sign Errors**: Implementing θ = θ + gradient instead of θ = θ - gradient causes ascent rather than descent—loss increases instead of decreasing. Similarly, forgetting negative sign in backprop chain rule terms. Always verify manually that gradients point toward loss decrease for a simple test case.",
    "**Numerical Underflow in Probabilities**: Very small probability values (1e-40) can underflow to zero, then log(0) = -∞. Work in log-space when possible. For example, store log-probabilities and use log-sum-exp trick for normalization. Or add epsilon smoothing to prevent exact zeros before taking logarithms."
  ],
  "quick_reference": [
    "**Forward Pass Formula**: A[l] = activation(W[l] · A[l-1] + b[l]) where A[0] = input, A[L] = output",
    "**Backprop Chain Rule**: ∂L/∂W[l] = (∂L/∂A[l]) · (∂A[l]/∂z[l]) · (∂z[l]/∂W[l]) = δ[l] · A[l-1]ᵀ",
    "**Error Signal**: δ[l] = (∂L/∂A[l]) ⊙ activation'(z[l]), propagate: δ[l-1] = (W[l]ᵀ · δ[l]) ⊙ activation'(z[l-1])",
    "**Gradient Descent Update**: θ_new = θ_old - learning_rate × ∇L(θ)",
    "**Common Activations**: σ(x)=1/(1+e⁻ˣ), tanh(x)=(eˣ-e⁻ˣ)/(eˣ+e⁻ˣ), ReLU(x)=max(0,x), softmax(x)ᵢ=eˣⁱ/Σⱼeˣʲ",
    "**Loss Functions**: MSE: L=(1/n)Σ(ŷ-y)², CrossEntropy: L=-Σy·log(ŷ), Binary: L=-[y·log(ŷ)+(1-y)·log(1-ŷ)]",
    "**Numerical Gradient**: ∇f(θ) ≈ (f(θ+ε) - f(θ-ε))/(2ε) for ε=1e-5, verify against analytical gradient",
    "**Xavier Init**: W ~ Uniform(-√(6/(fan_in+fan_out)), √(6/(fan_in+fan_out))) or Normal(0, √(2/(fan_in+fan_out)))",
    "**Shape Check Example**: Input[batch,n] × W1[n,h] → Hidden[batch,h] × W2[h,m] → Output[batch,m]",
    "**NaN Debug**: Print before suspected operation, check for log(0), exp(large), 0/0, inf-inf",
    "**Gradient Check**: relative_error = |analytical - numerical| / max(|analytical|, |numerical|) should be < 1e-7",
    "**Learning Rate Heuristic**: Start with 0.01, halve if loss oscillates, double if loss decreases too slowly",
    "**Common Derivatives**: d/dx(σ(x))=σ(x)(1-σ(x)), d/dx(tanh(x))=1-tanh²(x), d/dx(ReLU(x))=1 if x>0 else 0",
    "**Matrix Calculus**: ∂(Wx)/∂W = xᵀ, ∂(Wx)/∂x = Wᵀ, ∂(xᵀWy)/∂W = xyᵀ",
    "**Debug Checklist**: Shapes match? Gradients non-zero? Loss decreasing? NaNs present? Signs correct? Scale reasonable?"
  ],
  "resources": [
    {
      "filename": "mathematical_foundations.md",
      "description": "Core mathematical concepts required for neural network implementation—linear algebra operations, calculus derivatives, and probability fundamentals with neural network applications",
      "content": "# Mathematical Foundations for Neural Networks\n\n## Linear Algebra Essentials\n\n**Vectors and Matrices**\n- Vector: Ordered list of numbers, e.g., x = [x₁, x₂, ..., xₙ]\n- Matrix: 2D array of numbers, e.g., W = [[w₁₁, w₁₂], [w₂₁, w₂₂]]\n- Transpose: Flip rows/columns, Wᵀ swaps W[i,j] with W[j,i]\n\n**Matrix Multiplication**\n- (A × B)[i,j] = Σₖ A[i,k] · B[k,j]\n- Shape rule: [m,n] × [n,p] → [m,p] (inner dimensions must match)\n- Not commutative: A×B ≠ B×A in general\n- Neural network layer: output = input × weights, shape [batch, features] × [features, hidden] = [batch, hidden]\n\n**Key Operations**\n- Dot product: x·y = Σᵢ xᵢyᵢ (measures similarity/projection)\n- Element-wise (Hadamard) product: (x ⊙ y)ᵢ = xᵢyᵢ (used in gradient backprop)\n- Broadcasting: [batch, 1] + [batch, features] automatically expands to [batch, features]\n\n## Calculus for Optimization\n\n**Derivatives (Rate of Change)**\n- Derivative df/dx: How much f changes when x changes infinitesimally\n- Geometric interpretation: Slope of tangent line to f(x)\n- Neural networks use derivatives to find which direction decreases loss\n\n**Partial Derivatives**\n- For function f(x,y), ∂f/∂x is derivative with respect to x, treating y as constant\n- Neural networks have thousands of parameters—need partial derivative of loss with respect to each\n\n**Chain Rule (Core of Backpropagation)**\n- If y = f(g(x)), then dy/dx = (dy/dg) · (dg/dx)\n- Example: y = (x² + 1)³, let g = x² + 1, then dy/dx = 3g² · 2x = 6x(x² + 1)²\n- Neural network: loss depends on output, output depends on hidden layer, hidden depends on input\n- Chain rule links these: ∂loss/∂input = (∂loss/∂output) · (∂output/∂hidden) · (∂hidden/∂input)\n\n**Gradient**\n- Gradient ∇f = [∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ] is vector of all partial derivatives\n- Points in direction of steepest increase of f\n- Gradient descent: move in direction -∇f to decrease f\n\n**Common Derivatives for Neural Networks**\n- d/dx(xⁿ) = nxⁿ⁻¹\n- d/dx(eˣ) = eˣ\n- d/dx(log(x)) = 1/x\n- d/dx(sigmoid(x)) = sigmoid(x)(1 - sigmoid(x))\n- d/dx(tanh(x)) = 1 - tanh²(x)\n- d/dx(ReLU(x)) = 1 if x > 0, else 0\n\n## Probability Fundamentals\n\n**Random Variables and Distributions**\n- Random variable: Value determined by chance (e.g., coin flip, pixel intensity)\n- Distribution: Describes probability of different outcomes\n- Neural networks output probability distributions over classes\n\n**Expectation (Average)**\n- Expected value E[X] = Σᵢ xᵢP(xᵢ) for discrete, ∫x·p(x)dx for continuous\n- Neural networks minimize expected loss over training data\n\n**Likelihood**\n- P(data | model): Probability that model assigns to observed data\n- Training maximizes likelihood = minimizes negative log-likelihood (cross-entropy loss)\n\n**Key Concepts**\n- Independence: P(A,B) = P(A)P(B) when events don't affect each other\n- Conditional probability: P(A|B) = P(A,B)/P(B)\n- Bayes rule: P(A|B) = P(B|A)P(A)/P(B) (used in Bayesian neural networks)\n\n## Matrix Calculus for Neural Networks\n\n**Gradients of Matrix Operations**\n- ∂(Wx)/∂W = xᵀ (gradient of linear transformation w.r.t. weights)\n- ∂(Wx)/∂x = Wᵀ (gradient w.r.t. input—used in backprop)\n- ∂(xᵀWy)/∂W = xyᵀ (gradient of bilinear form)\n\n**Jacobian Matrix**\n- For vector function f: ℝⁿ → ℝᵐ, Jacobian J[i,j] = ∂fᵢ/∂xⱼ\n- Represents how each output component changes with each input component\n- In backprop, error propagates through Jacobian transposes\n\n## Practical Application\n\n**Neural Network as Composition of Functions**\n1. Linear transformation: z = Wx + b\n2. Nonlinear activation: a = σ(z)\n3. Another linear transformation: z' = W'a + b'\n4. Loss computation: L = loss(z', y_true)\n\n**Computing Loss Gradient**\n- Start with ∂L/∂z' (derivative of loss w.r.t. final output)\n- Chain rule: ∂L/∂W' = (∂L/∂z') · (∂z'/∂W') = (∂L/∂z') · aᵀ\n- Continue backward: ∂L/∂a = W'ᵀ · (∂L/∂z')\n- Apply activation derivative: ∂L/∂z = (∂L/∂a) ⊙ σ'(z)\n- Finally: ∂L/∂W = (∂L/∂z) · xᵀ\n\nAll neural network math reduces to these linear algebra and calculus operations applied systematically layer by layer."
    },
    {
      "filename": "implementation_guide.md",
      "description": "Step-by-step implementation instructions for building neural network components from scratch, including code patterns, testing strategies, and debugging techniques",
      "content": "# Neural Network Implementation From Scratch\n\n## Phase 1: Single Neuron\n\n**Objective**: Implement one computational unit to understand the atomic building block.\n\n**Code Structure**:\n```\nclass Neuron:\n    def __init__(self, n_inputs):\n        self.weights = random_small_values(n_inputs)  # e.g., uniform(-0.1, 0.1)\n        self.bias = 0.0\n    \n    def forward(self, inputs):\n        # inputs shape: [n_inputs]\n        # weights shape: [n_inputs]\n        z = dot_product(self.weights, inputs) + self.bias  # scalar\n        output = activation(z)  # e.g., sigmoid, ReLU\n        # Store for backward pass\n        self.last_input = inputs\n        self.last_z = z\n        self.last_output = output\n        return output\n    \n    def backward(self, d_output):\n        # d_output: gradient of loss w.r.t. this neuron's output\n        d_activation = activation_derivative(self.last_z)  # derivative of activation at z\n        d_z = d_output * d_activation  # local gradient\n        \n        # Gradients for parameters\n        self.d_weights = d_z * self.last_input  # shape: [n_inputs]\n        self.d_bias = d_z  # scalar\n        \n        # Gradient for input (to pass to previous layer)\n        d_input = d_z * self.weights  # shape: [n_inputs]\n        return d_input\n```\n\n**Testing**:\n- Input: [0.5, -0.3, 0.8], weights: [0.2, 0.4, -0.1], bias: 0.1\n- Compute forward pass by hand: z = 0.5×0.2 + (-0.3)×0.4 + 0.8×(-0.1) + 0.1 = 0.1 - 0.12 - 0.08 + 0.1 = 0.0\n- If activation is sigmoid: output = 1/(1+e⁰) = 0.5\n- Verify code produces 0.5\n\n## Phase 2: Layer Implementation\n\n**Objective**: Extend single neuron to matrix operations for efficiency.\n\n**Code Structure**:\n```\nclass Layer:\n    def __init__(self, n_inputs, n_neurons):\n        # Xavier initialization: scale by sqrt(1/n_inputs)\n        scale = sqrt(1.0 / n_inputs)\n        self.weights = random_normal(n_neurons, n_inputs) * scale  # [n_neurons, n_inputs]\n        self.bias = zeros(n_neurons)  # [n_neurons]\n    \n    def forward(self, inputs):\n        # inputs shape: [batch_size, n_inputs]\n        # weights shape: [n_neurons, n_inputs] needs transpose for matmul\n        # output shape: [batch_size, n_neurons]\n        \n        assert inputs.shape[1] == self.weights.shape[1], f\"Input features {inputs.shape[1]} != weight features {self.weights.shape[1]}\"\n        \n        z = matmul(inputs, self.weights.T) + self.bias  # broadcasting bias across batch\n        output = activation(z)  # element-wise\n        \n        # Store for backward pass\n        self.last_input = inputs\n        self.last_z = z\n        self.last_output = output\n        return output\n    \n    def backward(self, d_output):\n        # d_output shape: [batch_size, n_neurons]\n        batch_size = d_output.shape[0]\n        \n        # Local gradient through activation\n        d_activation = activation_derivative(self.last_z)  # [batch_size, n_neurons]\n        d_z = d_output * d_activation  # element-wise, [batch_size, n_neurons]\n        \n        # Gradient for weights: average gradient over batch\n        # d_weights shape: [n_neurons, n_inputs]\n        self.d_weights = matmul(d_z.T, self.last_input) / batch_size\n        \n        # Gradient for bias: average over batch\n        self.d_bias = mean(d_z, axis=0)  # [n_neurons]\n        \n        # Gradient for input: pass to previous layer\n        # d_input shape: [batch_size, n_inputs]\n        d_input = matmul(d_z, self.weights)\n        \n        return d_input\n```\n\n**Testing**:\n- Create layer with 3 inputs, 2 neurons\n- Input batch: [[1, 0, -1], [0.5, 0.5, 0]] (2 samples, 3 features)\n- Initialize weights and bias to known values\n- Compute forward pass by hand for first sample\n- Verify code output matches hand calculation\n- Check output shape is [2, 2] (batch_size, n_neurons)\n\n## Phase 3: Loss Functions\n\n**Mean Squared Error (Regression)**:\n```\ndef mse_loss(predictions, targets):\n    # predictions, targets shape: [batch_size, n_outputs]\n    diff = predictions - targets\n    loss = mean(diff ** 2)\n    return loss\n\ndef mse_loss_derivative(predictions, targets):\n    # Gradient of MSE w.r.t. predictions\n    batch_size = predictions.shape[0]\n    d_loss = 2 * (predictions - targets) / batch_size\n    return d_loss\n```\n\n**Cross-Entropy (Classification)**:\n```\ndef cross_entropy_loss(predictions, targets):\n    # predictions: [batch_size, n_classes] (softmax probabilities)\n    # targets: [batch_size, n_classes] (one-hot encoded)\n    epsilon = 1e-10  # prevent log(0)\n    predictions_clipped = clip(predictions, epsilon, 1 - epsilon)\n    loss = -mean(sum(targets * log(predictions_clipped), axis=1))\n    return loss\n\ndef cross_entropy_derivative(predictions, targets):\n    # When combined with softmax, simplifies to:\n    batch_size = predictions.shape[0]\n    d_loss = (predictions - targets) / batch_size\n    return d_loss\n```\n\n**Testing**:\n- Perfect prediction: predictions = targets should give loss ≈ 0\n- Worst prediction: predictions = 1 - targets should give high loss\n- Check gradient: small change in predictions should change loss by approximately gradient × change\n\n## Phase 4: Activation Functions\n\n**Implementation with Derivatives**:\n```\ndef sigmoid(x):\n    return 1 / (1 + exp(-x))\n\ndef sigmoid_derivative(x):\n    s = sigmoid(x)\n    return s * (1 - s)\n\ndef relu(x):\n    return maximum(0, x)  # element-wise max\n\ndef relu_derivative(x):\n    return (x > 0).astype(float)  # 1 where x > 0, else 0\n\ndef tanh(x):\n    return (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n\ndef tanh_derivative(x):\n    t = tanh(x)\n    return 1 - t ** 2\n\ndef softmax(x):\n    # Numerical stability: subtract max before exp\n    # x shape: [batch_size, n_classes]\n    x_shifted = x - max(x, axis=1, keepdims=True)\n    exp_x = exp(x_shifted)\n    return exp_x / sum(exp_x, axis=1, keepdims=True)\n```\n\n**Testing**:\n- sigmoid(0) = 0.5, sigmoid(large) → 1, sigmoid(-large) → 0\n- ReLU: negative inputs give 0, positive pass through\n- softmax: outputs sum to 1, all values in [0,1]\n\n## Phase 5: Gradient Checking\n\n**Numerical Gradient Implementation**:\n```\ndef numerical_gradient(loss_function
Philosopher's Stone v4 × Skill Forge × EchoSeed
